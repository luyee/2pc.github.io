---
layout: post
title: "Logistic回归原理公式整理"
keywords: ["ML","mechine learning","SVM","","Dtree","classification"]
description: "svm"
category: "Datascience"
tags: ["ML","Datascience"]
---


## Logistic函数（或者称 Sigmoid函数）

$$
g(z)=\frac 1 {1+e^{(-z)}}
$$

令

$$
z=\theta_0 + \theta_1 x_1+...+\theta_n x_n=\sum_{i=0}^n (\theta_i x_i)=\theta^{(T)}x
$$

则

$$
g(z)=g(\theta^{T}x)=\frac 1 {1+e^{(-\theta^{T}x)}}＝h_\theta(x)
$$

函数$$h_\theta(x)$$的含义，表示结果取1 时的概率，对于输入x，分类结果为类别0和类别1的概率，可表示为：

$$
P(y=1|x;\theta)=h_\theta(x)
$$

$$
P(y=0|x;\theta)=1-h_\theta(x)
$$
假设现在有m个相互独立的观察事件$$y(y^{(1)},y^{(2)},y^{(3)},...,y^{(m)})$$,  则一个事件$$y^{(i)}$$发生的概率为($$y^{(i)}=1$$)

$$
p(y^{(i)})=p^{y^{(i)}} {(1-p)}^{1-y^{(i)}}
$$

即

$$
p(y|x;\theta)=h_\theta(x)^{y^{(i)}}(1-h_\theta(x))^{1-y^{(i)}}
$$

似然函数

$$
L(\theta)=\prod_{j=0}^m (h_\theta(x_i)^{y_i}*(1-h_\theta(x_i))^{1-y_i})
$$

 对数似然函数
$$
l(\theta)=log(L(\theta))=\sum_1^m(y_ilogh_\theta(x_i)+(1-y_i)log(1-h_\theta(x_i)))
$$23333

loss(Cost)函数

$$
 J(\theta)=-\frac 1 m l(\theta)
$$

结合梯度下降法求最小值时的公式

$$
\theta_i = \theta_i - \alpha\frac\partial{\partial\theta_i}J(\theta)
$$

则

$$
$$ \frac\partial{\partial\theta_j}J(\theta)＝-\frac 1 m \sum_1^m{((y_i \frac 1 {h_\theta(x)})}{\frac\partial{\partial\theta_j}{h_\theta(x)}}-{(1-y_i)\frac 1 {(1-h_\theta(x))}}{\frac\partial{\partial\theta_j}{h_\theta(x)})} 
$$

$$\frac\partial{\partial\theta_j}J(\theta)$$求导数

$$
\begin {aligned}  
\frac\partial{\partial\theta_j}J(\theta)\\\
&＝ -\frac 1 m \sum_1^m{y_i* \frac 1 {h_\theta(x_i)}}{\frac\partial{\partial\theta_j}{h_\theta(x_i)}}-{(1-y_i)\frac 1 {(1-h_\theta(x_i))}}{\frac\partial{\partial\theta_j}{h_\theta(x_i)}}\\\
&=-\frac 1 m \sum_1^m({y_i* \frac 1 {g(\theta^{T}x_i)}}-{(1-y_i)\frac 1 {(1-g(\theta^{T}x_i))}}){\frac\partial{\partial\theta_j}{g(\theta^{T}x_i)}}\\\
&=-\frac 1 m \sum_1^m({y_i* \frac 1 {g(\theta^{T}x_i)}}-{(1-y_i)\frac 1 {(1-g(\theta^{T}x_i))}}){g(\theta^{T}x_i){(1-{g(\theta^{T}x_i))}}}{\frac\partial{\partial\theta_j}{(\theta^{T}{x_i})}}\\\
&=-\frac 1 m \sum_1^m{(({y_i*{(1-g(\theta^{T}x_i)}})-{(1-y_i)}{g(\theta^{T}x_i})})x_i^j\\\
&=-\frac 1 m \sum_1^m{{(y_i-g(\theta^{T}x_i))}}x_i^j\\\
&=-\frac 1 m \sum_1^m{{(y_i-h_\theta(x_i))}}x_i^j\\\
\end {aligned}
$$
Over
